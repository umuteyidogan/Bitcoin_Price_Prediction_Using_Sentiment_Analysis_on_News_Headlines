{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02625d80",
   "metadata": {},
   "source": [
    "# Data cleaning script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf8614",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "851ee43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "# Uncomment if not downloaded to your device\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe4d53",
   "metadata": {},
   "source": [
    "### Upload csv file\n",
    "We are using data from 2 sources; Nexus and Kaggle. The Nexus data is already fairly clean as it allowed us to choose which columns we required and filter the data before downloading it to a csv. \n",
    "\n",
    "The Kaggle data requires a bit more cleaning and reorganising before we are able to concatonate the data sets together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "78b43ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload Nexus csv \n",
    "Nexus_csv_path = 'Headline_data_v2.csv'\n",
    "Nexus_data_raw = pd.read_csv(Nexus_csv_path, encoding='windows-1252')\n",
    "\n",
    "#Upload Kaggle csv\n",
    "Kaggle_csv_path = 'Kaggle data.csv'\n",
    "Kaggle_data_raw = pd.read_csv(Kaggle_csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36321599",
   "metadata": {},
   "source": [
    "## Restructuring Kaggle data \n",
    "The Kaggle data has extra information on the price of Bitcoin. This information is not given in the Nexus data set and to keep the consistency it is better to get the Bitcoin price information from the same source to cover teh entire data set. This means these collumns can be dropped. The symbol column can also be dropped the whole column is 'BTC' as an entry which doesn't provide much insite. \n",
    "\n",
    "The kaggle data has multiple headlines per date. This needs to be restructured to 1 headline per row.\n",
    "\n",
    "The collumn also need to be renamed to be consistent with the Nexus data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "39d28c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns \n",
    "Kaggle_data = Kaggle_data_raw.drop(columns=['Unnamed: 0', 'open_price', 'close_price','high_price', 'low_price', 'symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5cd0b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates each headline onto a new row.\n",
    "Kaggle_data['articles'] = Kaggle_data['articles'].apply(lambda x: x.split(\"', \"))\n",
    "Kaggle_data = Kaggle_data.explode('articles').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0c5525bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename collumns in order to combine with Nexus data\n",
    "Kaggle_data = Kaggle_data.rename(columns={'begins_at': 'Published date', 'articles': 'Headline'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fad52",
   "metadata": {},
   "source": [
    "## Removing non-English headlines\n",
    "When downloading the Nexus data we were able to pick parameters for the data we wanted, this meant we could select only English language papers. For the Kaggle data there is a mixture of different languages. As we are only focusing on English language papers these need to be removed. Dectecting language takes a lot of computing power so it is important to do this step before combining with the Nexus data. \n",
    "\n",
    "To remove them, a new column is created which has the detected language in it. If the language is not detected it is set to 'unknown', this prevents error messages. Then only the 'en' language papers are kept in the data frame and the lang collumn is dropped as it is no longer needed. \n",
    "\n",
    "Originally when running the code there was a SettingWithCopyWarning. To avoid this warning .loc has been used to access the lang column. For further information on SettingWithCopyWarning 'https://www.analyticsvidhya.com/blog/2021/11/3-ways-to-deal-with-settingwithcopywarning-in-pandas/#:~:text='SettingWithCopy'%20is%20a%20common%20warning,settingwithcopywarning%20new%20column%20pops%20up'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c3571b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>['Original Pizza Day Purchaser Does It Again W...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'This 11-year-old just wrote a book on bitcoin...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'Without Mentioning Blockchain, Putin Says Tha...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'El comprador original del Pizza Day lo hace d...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'Meet the strippers tattooed with BARCODES so ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Published date                                           Headline lang\n",
       "0     25/02/2018  ['Original Pizza Day Purchaser Does It Again W...   en\n",
       "1     25/02/2018  'This 11-year-old just wrote a book on bitcoin...   en\n",
       "2     25/02/2018  'Without Mentioning Blockchain, Putin Says Tha...   en\n",
       "3     25/02/2018  'El comprador original del Pizza Day lo hace d...   es\n",
       "4     25/02/2018  'Meet the strippers tattooed with BARCODES so ...   en"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dectect language of newspaper \n",
    "from langdetect import detect\n",
    "\n",
    "# Safe detect for when the language is not detecable\n",
    "def safe_detect(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# This function takes a long time to perform\n",
    "# Use .loc to set the new column to avoid SettingWithCopyWarning\n",
    "Kaggle_data.loc[:, 'lang'] = Kaggle_data['Headline'].apply(lambda x: safe_detect(x))\n",
    "Kaggle_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "11010ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English headlines\n",
    "Kaggle_data = Kaggle_data[Kaggle_data['lang'] == 'en']\n",
    "Kaggle_data = Kaggle_data.drop('lang', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbf5b3",
   "metadata": {},
   "source": [
    "## Remove headlines with CRYPTO[...]World Markets\n",
    "There are a lot of news headlines in the nexus data with this structure and they do not provide any extra information so it is best ot remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "837201e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'CRYPTO.*World Markets'\n",
    "\n",
    "# Filter out rows containing the specific pattern\n",
    "Nexus_data = Nexus_data_raw[~Nexus_data_raw['Headline'].str.contains(pattern, regex=True, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91416474",
   "metadata": {},
   "source": [
    "## Combining data into a single data frame.\n",
    "The Kaggle data has been reshaped and renamed so that is is now compatible to combine with the Nexus data. The Kaggle data only contains the publish date and the headline, whereas the nexus data has 2 other columns. This will just mean that for Kaggle data there will be an NA for entries in these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e6d182cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19364\n",
      "19364\n"
     ]
    }
   ],
   "source": [
    "headline_data = pd.concat([Nexus_data_raw, Kaggle_data])\n",
    "\n",
    "#Checks\n",
    "print(len(Nexus_data_raw) + len(Kaggle_data))\n",
    "print(len(headline_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d84c4",
   "metadata": {},
   "source": [
    "## Cleaning data frame.\n",
    "The duplicate rows are removed as this could alter the senitment analysis if there are multiple entries for one headline. The null headlines are also removevd as they provide no extra information. \n",
    "\n",
    "The data then needs to be cleaned specifically for sentiment analysis. Headlings are lower cased and unctuation removed to reduce noise and so that the model can focus on the more meaningful parts of the headline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a848d",
   "metadata": {},
   "source": [
    "#### Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b7b5b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dulpicate rows\n",
    "headline_data.drop_duplicates(subset=['Headline'], keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40d64e",
   "metadata": {},
   "source": [
    "#### Remove Null headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e7fbad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published date</th>\n",
       "      <th>Countries</th>\n",
       "      <th>Company</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30/05/2024</td>\n",
       "      <td>England &amp; Wales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Woman jailed for Bitcoin laundering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06/06/2024</td>\n",
       "      <td>England &amp; Wales</td>\n",
       "      <td>BEST INC</td>\n",
       "      <td>Best Crypto Casino Sites &amp; Bitcoin Casinos in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21/05/2024</td>\n",
       "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wright lied extensively as 'bitcoin inventor'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26/05/2024</td>\n",
       "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
       "      <td>METROPOLITAN BANK HOLDING CORP</td>\n",
       "      <td>Ex-takeaway worker with Bitcoin worth more tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21/05/2024</td>\n",
       "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
       "      <td>BIRD &amp; BIRD LLP</td>\n",
       "      <td>Inventor of bitcoin' given court rebuke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Published date                                          Countries  \\\n",
       "0     30/05/2024                                    England & Wales   \n",
       "1     06/06/2024                                    England & Wales   \n",
       "2     21/05/2024  United Kingdom of Great Britain and Northern I...   \n",
       "3     26/05/2024  United Kingdom of Great Britain and Northern I...   \n",
       "4     21/05/2024  United Kingdom of Great Britain and Northern I...   \n",
       "\n",
       "                          Company  \\\n",
       "0                             NaN   \n",
       "1                        BEST INC   \n",
       "2                             NaN   \n",
       "3  METROPOLITAN BANK HOLDING CORP   \n",
       "4                 BIRD & BIRD LLP   \n",
       "\n",
       "                                            Headline  \n",
       "0                Woman jailed for Bitcoin laundering  \n",
       "1  Best Crypto Casino Sites & Bitcoin Casinos in ...  \n",
       "2      Wright lied extensively as 'bitcoin inventor'  \n",
       "3  Ex-takeaway worker with Bitcoin worth more tha...  \n",
       "4            Inventor of bitcoin' given court rebuke  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes all missing headlines\n",
    "headline_data.dropna(subset=['Headline'])\n",
    "headline_data['Headline'] = headline_data['Headline'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fac946",
   "metadata": {},
   "source": [
    "#### Lower case headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "60fd085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case headlines\n",
    "# Loops through every word in the the Title collumn of the data frame and applies the lower() function\n",
    "# The lower function only works on strings, so the Title collumn must be split up first and then rejoined again after. \n",
    "headline_data['Cleaned Headline'] = headline_data['Headline'].apply(lambda x: ' '.join(word.lower() for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691fc22",
   "metadata": {},
   "source": [
    "#### Remove punctuation\n",
    "We don't want to remove punctuation entirely as Vader and TextBlob can process and use punction to help. This is a little clean up to standardise the punctuation in the head line. I have added a space between any punctuation and words, this is because to the program punctuation changed the string e.g. if we wanted to remove the stop word if, 'if' and 'if,' are not read as equavalient strings and so the word isn't removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a4f5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "# Strip \\n \n",
    "# This symbol indicates a new line. As it involves a punctuation symbol, this step must be done before removing punctuation.\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "# Remove anything after the '|'. This si because it is jsut the name of the newspaper and not the headline.\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].str.split('|').str[0]\n",
    "\n",
    "#This separates the words from punctuation this is important to stop \n",
    "#'bitcoin bitcoin, and bitcoin being counted as different words. \n",
    "def add_spaces_around_punctuation(text):\n",
    "    # Define a regex pattern for punctuation\n",
    "    pattern = r'([.,!?;:()\\'\"\\[\\]{}<>@#$%^&*+=~`|\\\\/-])'\n",
    "    # Use re.sub to add spaces around each punctuation mark\n",
    "    spaced_text = re.sub(pattern, r' \\1 ', text)\n",
    "    # Remove any extra spaces that might appear\n",
    "    spaced_text = re.sub(r'\\s+', ' ', spaced_text).strip()\n",
    "    return spaced_text\n",
    "\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].apply(add_spaces_around_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762f177",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "Stop words are a set of commonly used words in a language. In the context of NLP, they carry very little meaning and so are removed to further reduce noise. \n",
    "\n",
    "The library of stop words from the nltk module is used to initially remove stop words. Then there is a count of most commonly occuring words in the data set. From there more specific to crypto stop words can be picked out and removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "30b55885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "# Imported list of English stop words for NLTK\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Loops through and if word not in the stop words list then it is kept in the headline. (all words in stop word list removed)\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73676e60",
   "metadata": {},
   "source": [
    "### Dectecting more specific stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a56f3095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin           12519\n",
      "btc                2712\n",
      "crypto             2264\n",
      "price              2004\n",
      "new                 783\n",
      "ethereum            747\n",
      "says                709\n",
      "market              686\n",
      "cryptocurrency      683\n",
      "mining              644\n",
      "analysis            542\n",
      "could               469\n",
      "us                  425\n",
      "million             421\n",
      "time                395\n",
      "high                381\n",
      "first               378\n",
      "year                365\n",
      "eth                 354\n",
      "trading             343\n",
      "blockchain          337\n",
      "buy                 322\n",
      "exchange            320\n",
      "money               302\n",
      "world               295\n",
      "gold                291\n",
      "markets             285\n",
      "top                 282\n",
      "investors           278\n",
      "xrp                 263\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create our own list of stop words. The imported stop words are a good start but not specific to crypto headline data.\n",
    "\n",
    "# Check frequency of words.\n",
    "\n",
    "# Combine all headlines into a single string and split into words\n",
    "all_words = \" \".join(headline_data['Cleaned Headline']).split()\n",
    "\n",
    "# Create a pandas Series from the words\n",
    "words_series = pd.Series(all_words)\n",
    "\n",
    "# Filter out non-alphabetic words and count occurrences\n",
    "word_counts = words_series[words_series.str.isalpha()].value_counts()[:30]\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608e8da",
   "metadata": {},
   "source": [
    "### Selecting specific stop words to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5a4f4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is very subjective. \n",
    "# It is just deciding which words aren't as relavent to help reduce noise in the sentiment analysis\n",
    "manual_stop_words = ['us', 'says', 'sam', 'sec', 'could', 'bankmanfried', 'man', 'wall', 'woman', 'briefing', 'amid', 'lucy', 'know']\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].apply(lambda x: ' '.join(word for word in x.split() if word not in manual_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abfe58",
   "metadata": {},
   "source": [
    "### Changing abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e7bacb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the word freq I noticed 5bn. I am going to separate numbers and replace bn with billion and mn with million.\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].str.replace(r\"bn\", r\" billion\" , regex=True).replace(r\"mn\", r\" million\" , regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba345a88",
   "metadata": {},
   "source": [
    "## Headline relevance checker\n",
    "Due to the sheer number of headlines it is too time consuming to go through every one to decide if they are relavent to bitcoin or not. To get around this a new column is added to the data frame that has the number of crypto relavent words in the headline. \n",
    "\n",
    "There were no suitable pre made list of crypto relavent words online. This mean we have to scrap the website 'https://cryptonest.co.uk/pages/crypto-dictionary' for all their crypto definition using beautiful soup. All the definition words were within the class 'page-content page-content--medium rte' and written in bold so it was easy to locate and isolate only the key words. \n",
    "\n",
    "Challenges arose as some key words had their corresponing abbreviation in brackets e.g Ether (ETH). Some keywords had a slash in them to indicate similar words e.g. bear / bearish. This meant that the string to compare to the headline was with key word and abbreviaiton. These needed to be separated out into two different key words so that they key word and abbreviaiton could be compared separately. \n",
    "\n",
    "A set was used to make sure there were no duplicate words in the keywords. This was then converted back into a list as sets are immutable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5a8c6",
   "metadata": {},
   "source": [
    "#### Scraping website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6dfa0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Website with crypto dictionary\n",
    "url = 'https://cryptonest.co.uk/pages/crypto-dictionary' \n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Class containing all the definitions in\n",
    "class_name = \"page-content page-content--medium rte\" \n",
    "\n",
    "# Find all elements with the specified class\n",
    "elements_with_class = soup.find_all(class_=class_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77776c3f",
   "metadata": {},
   "source": [
    "#### Cleaning Key words to be compatible with headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8f1305b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to remove all punctuation except brackets\n",
    "\n",
    "# Initialize a set to store cleaned words\n",
    "crypto_words_set = set()\n",
    "\n",
    "# Regular expression to find text within brackets\n",
    "brackets_pattern = re.compile(r'\\(([^)]+)\\)')\n",
    "\n",
    "# Iterate over elements and find all <strong> tags within each\n",
    "for element in elements_with_class:\n",
    "    strong_tags = element.find_all('strong')\n",
    "    for key_words in strong_tags:\n",
    "        # Get the text and convert to lowercase\n",
    "        text = key_words.get_text().lower()\n",
    "        \n",
    "        # Remove hyphens \n",
    "        text = text.replace('-', ' ')\n",
    "        \n",
    "        # Extract bracketed words\n",
    "        bracketed_words = brackets_pattern.findall(text)\n",
    "        \n",
    "        # Remove bracketed words from the original text\n",
    "        text_without_brackets = re.sub(brackets_pattern, '', text)\n",
    "        \n",
    "        # Process original text without bracketed parts\n",
    "        split_words = text_without_brackets.split('/')\n",
    "        \n",
    "        # Add non-bracketed words to the set\n",
    "        for word in split_words:\n",
    "            cleaned_word = word.strip()\n",
    "            if cleaned_word:\n",
    "                crypto_words_set.add(cleaned_word)\n",
    "        \n",
    "        # Add bracketed words separately to the set\n",
    "        for word in bracketed_words:\n",
    "            cleaned_word = word.strip()\n",
    "            if cleaned_word:\n",
    "                crypto_words_set.add(cleaned_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aae628",
   "metadata": {},
   "source": [
    "#### Adding missed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "00b6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scanning the list crypto and bitcoins werent included so manually adding it to the set.\n",
    "# Bitcoins doesn't lemmatise to bitcoin I tihnk this is because it is specific to crypto.\n",
    "crypto_words_set.add('bitcoins')\n",
    "\n",
    "# Convert to a list so it can be used in function below\n",
    "crypto_words = list(crypto_words_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a4da2",
   "metadata": {},
   "source": [
    "#### Adding count collumn for number of crypto relavent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0b2049d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some words contain crypto e.g. cryptoqueen were not being picked up. This ensures all variations are counted\n",
    "containing_crypto = re.compile(r'\\b\\w*crypto\\w*\\b')\n",
    "containing_coin = re.compile(r'\\b\\w*coin\\w*\\b')\n",
    "\n",
    "# Apply the function to calculate the count\n",
    "headline_data['Count'] = headline_data['Cleaned Headline'].apply(\n",
    "    lambda x: sum(1 for word in x.split() if containing_crypto.search(word) or containing_coin.search(word) or word in crypto_words)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3c5e6",
   "metadata": {},
   "source": [
    "#### Selecting only the relavent headlines.\n",
    "A csv is made of both relavent and non-relavent so they can be skimmed through to double check if the are relavent or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f6613878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv of all the non-relavent headlines which can be double checked to make sure they are not relavent. \n",
    "countdf = headline_data[headline_data['Count'] == 0]\n",
    "countdf.to_csv('Count_0.csv')\n",
    "\n",
    "# Create a csv of all the relavent headlines to double check they are relavent. \n",
    "crypto_headline_data = headline_data[headline_data['Count'] != 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76413e6b",
   "metadata": {},
   "source": [
    "### Lemmatising words\n",
    "Lemmatising is the process to reverting words back to their root. This process is requires a lot of computing power so best to do it at the end when there is only the relavent headlines in their cleanest form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "30258b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josie\\AppData\\Local\\Temp\\ipykernel_19676\\134444803.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  crypto_headline_data.loc[:, 'Lem Headline'] = crypto_headline_data['Cleaned Headline']\\\n"
     ]
    }
   ],
   "source": [
    "# Import textblob\n",
    "from textblob import Word\n",
    "\n",
    "# Lemmatise final review format\n",
    "crypto_headline_data.loc[:, 'Lem Headline'] = crypto_headline_data['Cleaned Headline']\\\n",
    ".apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "crypto_headline_data.to_csv('crypto_headline_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf71a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
